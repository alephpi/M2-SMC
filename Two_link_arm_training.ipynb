{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c40416-2a80-48a1-9da6-b8287cb47cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, autograd as ag\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "        'n_tasks': 1000,\n",
    "        'n_episode': 150,\n",
    "        'n_timesteps': 10,\n",
    "        'input_dim': 2,\n",
    "        'hidden_dim': 512,\n",
    "        'output_dim': 2,\n",
    "        'lr': 1e-4,\n",
    "        'batch_size': 128,\n",
    "        'n_epochs': 100,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiTA-Cwr5Pey"
   },
   "source": [
    "# Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "\t# `set_seed=None` means no seed fixed.  \n",
    "\tdef __init__(self, n_tasks=hyperparams['n_tasks'], n_episode=hyperparams['n_episode'], \n",
    "\t\t\t\tn_timesteps=hyperparams['n_timesteps'], set_seed: Union[int, None]=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.q = []\n",
    "\t\tself.x = []\n",
    "\t\tself.z = []\n",
    "\t\t# if set_seed is not None:\n",
    "\t\t# \tnp.random.seed(set_seed)\n",
    "\t\tl_of_tasks = np.random.normal(loc=1, scale=0.3, size=(n_tasks,2))\n",
    "\t\tfor i in tqdm(range(n_tasks)):\n",
    "\t\t\tl = l_of_tasks[i]\n",
    "\t\t\tfor _ in range(n_episode):\n",
    "\t\t\t\tq_, x, z = TrainingDataset.generate_episode(l, n_timesteps)\n",
    "\t\t\t\tself.q.extend(q_)\n",
    "\t\t\t\tself.x.extend(x)\n",
    "\t\t\t\tself.z.extend(z)\n",
    "\t\tself.q = np.stack(self.q, axis=1).reshape(-1,2)\n",
    "\t\tself.x = np.stack(self.x, axis=1).reshape(-1,2)\n",
    "\t\tself.z = np.stack(self.z, axis=1).reshape(-1,2)\n",
    "\t\tassert self.q.shape == self.x.shape\n",
    "\t\tprint(\"Two Arm Link Dataset Generation Finished!\")\n",
    "\t\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tq = self.q[index]\n",
    "\t\tx = self.x[index]\n",
    "\t\tz = self.z[index]\n",
    "\t\tsample = {\n",
    "\t\t\t\"angle\": torch.tensor(q),\n",
    "\t\t\t\"true_pos\": torch.tensor(x),\n",
    "\t\t\t\"noisy_pos\": torch.tensor(z),\n",
    "\t\t}\n",
    "\t\treturn sample\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.q)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef generate_episode(l, n_timesteps):\n",
    "\t\tdef x_of_q(q, l):\n",
    "\t\t\t# q: array (2,) containing the two angles\n",
    "\t\t\t# l: array (2,) containing the limb lengths \n",
    "\t\t\tx = l[0] * np.cos(q[0]) + l[1] * np.cos(q[0] + q[1])\n",
    "\t\t\ty = l[0] * np.sin(q[0]) + l[1] * np.sin(q[0] + q[1])\n",
    "\t\t\treturn np.array([x, y])\n",
    "\n",
    "\t\tq_0 = np.random.uniform(low= -np.pi, high= np.pi, size=(2,1))\n",
    "\t\tu = np.random.randn(2, n_timesteps-1)\n",
    "\t\tq = u.cumsum(axis=1) + q_0\n",
    "\t\tq = np.concatenate((q_0, q), axis=1)\n",
    "\t\tx = x_of_q(q, l=l)\n",
    "\t\tz = x + np.random.normal(0, scale=0.001, size=(2, n_timesteps))\n",
    "\t\tassert q.shape == x.shape\n",
    "\t\treturn q, x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size=hyperparams['batch_size'], n_tasks=hyperparams['n_tasks'], \n",
    "                set_seed: Union[int, None]=None, shuffle: bool=True):\n",
    "    dataset = TrainingDataset(n_tasks=n_tasks, set_seed=set_seed)\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=True,\n",
    "                            )\n",
    "    return data_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433695f6-b72d-4a99-b13e-9d280b5e8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim=hyperparams['hidden_dim']):\n",
    "        super(DropoutLayer, self).__init__()\n",
    "        self.mask = torch.ones(hidden_dim, dtype=torch.float32, requires_grad=False)\n",
    "        self.training = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.mask\n",
    "\n",
    "    def update(self, mask: np.ndarray):\n",
    "        assert mask.shape == self.mask.shape, f\"new mask shape should be {self.mask.shape} but giving {mask.shape}\"\n",
    "        self.mask = torch.Tensor(mask)\n",
    "        \n",
    "    def get(self):\n",
    "        return self.mask.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim=hyperparams['input_dim'], hidden_dim=hyperparams['hidden_dim'], \n",
    "                output_dim=hyperparams['output_dim']):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.lin2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.lin3 = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # dropout for pretraining and finetunning\n",
    "        self.dropout_for_GD = nn.Dropout(p=0.5)\n",
    "        # dropout for smc\n",
    "        self.dropout = DropoutLayer(hidden_dim=hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        if self.training:\n",
    "            x = self.dropout_for_GD(x)\n",
    "        else:\n",
    "            x = self.dropout(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "        \n",
    "    def update_dropout_mask(self, mask: np.ndarray):\n",
    "        self.dropout.update(mask)\n",
    "\n",
    "    def get_dropout_mask(self):\n",
    "        return self.dropout.get()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 146.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Arm Link Dataset Generation Finished!\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_loader(n_tasks=1000, set_seed=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyModel().to(device)\n",
    "model = model.double()\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['lr'])\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_task(data_loader=train_loader, nb_epochs=hyperparams['n_epochs']):\n",
    "    total_loss = []\n",
    "    for epoch in range(1, nb_epochs + 1):\n",
    "    \t# store the loss in each batch\n",
    "        losses = []\n",
    "        # use tqdm to better visualize the training process\n",
    "        with tqdm(data_loader, unit=\"batch\") as tepoch: # evaluate every batch\n",
    "            for data in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\") # custome the printed message in tqdm\n",
    "                x = data['angle'].to(device)\n",
    "                y = data['true_pos'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model.forward(x)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward() # backward propagation\n",
    "                optimizer.step() # update parameters\n",
    "                losses.append(loss.item())\n",
    "                # custome what is printed in tqdm message\n",
    "                tepoch.set_postfix(loss=sum(losses)/len(losses))\n",
    "        total_loss.append(sum(losses)/len(losses))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 11718/11718 [04:24<00:00, 44.37batch/s, loss=0.256]\n",
      "Epoch 2: 100%|██████████| 11718/11718 [04:28<00:00, 43.66batch/s, loss=0.168]\n",
      "Epoch 3: 100%|██████████| 11718/11718 [04:38<00:00, 42.03batch/s, loss=0.153]\n",
      "Epoch 4: 100%|██████████| 11718/11718 [04:34<00:00, 42.71batch/s, loss=0.146]\n",
      "Epoch 5: 100%|██████████| 11718/11718 [04:49<00:00, 40.52batch/s, loss=0.142]\n",
      "Epoch 6: 100%|██████████| 11718/11718 [04:37<00:00, 42.19batch/s, loss=0.139]\n",
      "Epoch 7: 100%|██████████| 11718/11718 [04:45<00:00, 41.07batch/s, loss=0.137]\n",
      "Epoch 8: 100%|██████████| 11718/11718 [04:49<00:00, 40.52batch/s, loss=0.135]\n",
      "Epoch 9: 100%|██████████| 11718/11718 [04:41<00:00, 41.69batch/s, loss=0.134]\n",
      "Epoch 10: 100%|██████████| 11718/11718 [04:51<00:00, 40.14batch/s, loss=0.133]\n",
      "Epoch 11: 100%|██████████| 11718/11718 [04:48<00:00, 40.66batch/s, loss=0.132]\n",
      "Epoch 12: 100%|██████████| 11718/11718 [05:02<00:00, 38.75batch/s, loss=0.131]\n",
      "Epoch 13: 100%|██████████| 11718/11718 [04:50<00:00, 40.39batch/s, loss=0.13]\n",
      "Epoch 14: 100%|██████████| 11718/11718 [05:04<00:00, 38.50batch/s, loss=0.13]\n",
      "Epoch 15: 100%|██████████| 11718/11718 [04:50<00:00, 40.34batch/s, loss=0.129]\n",
      "Epoch 16: 100%|██████████| 11718/11718 [04:54<00:00, 39.83batch/s, loss=0.129]\n",
      "Epoch 17: 100%|██████████| 11718/11718 [04:59<00:00, 39.12batch/s, loss=0.129]\n",
      "Epoch 18: 100%|██████████| 11718/11718 [04:40<00:00, 41.81batch/s, loss=0.128]\n",
      "Epoch 19: 100%|██████████| 11718/11718 [04:11<00:00, 46.51batch/s, loss=0.128]\n",
      "Epoch 20: 100%|██████████| 11718/11718 [03:30<00:00, 55.69batch/s, loss=0.127]\n",
      "Epoch 21: 100%|██████████| 11718/11718 [03:28<00:00, 56.16batch/s, loss=0.127]\n",
      "Epoch 22: 100%|██████████| 11718/11718 [03:36<00:00, 54.18batch/s, loss=0.127]\n",
      "Epoch 23: 100%|██████████| 11718/11718 [03:36<00:00, 54.04batch/s, loss=0.127]\n",
      "Epoch 24: 100%|██████████| 11718/11718 [03:33<00:00, 54.95batch/s, loss=0.126]\n",
      "Epoch 25: 100%|██████████| 11718/11718 [03:31<00:00, 55.29batch/s, loss=0.126]\n",
      "Epoch 26: 100%|██████████| 11718/11718 [03:39<00:00, 53.31batch/s, loss=0.126]\n",
      "Epoch 27: 100%|██████████| 11718/11718 [03:38<00:00, 53.66batch/s, loss=0.126]\n",
      "Epoch 28: 100%|██████████| 11718/11718 [03:33<00:00, 54.95batch/s, loss=0.126]\n",
      "Epoch 29: 100%|██████████| 11718/11718 [03:36<00:00, 54.16batch/s, loss=0.125]\n",
      "Epoch 30: 100%|██████████| 11718/11718 [03:36<00:00, 54.04batch/s, loss=0.125]\n",
      "Epoch 31: 100%|██████████| 11718/11718 [03:37<00:00, 53.84batch/s, loss=0.125]\n",
      "Epoch 32: 100%|██████████| 11718/11718 [03:37<00:00, 53.98batch/s, loss=0.125]\n",
      "Epoch 33: 100%|██████████| 11718/11718 [03:37<00:00, 53.80batch/s, loss=0.125]\n",
      "Epoch 34: 100%|██████████| 11718/11718 [03:40<00:00, 53.11batch/s, loss=0.125]\n",
      "Epoch 35: 100%|██████████| 11718/11718 [03:34<00:00, 54.62batch/s, loss=0.125]\n",
      "Epoch 36: 100%|██████████| 11718/11718 [03:37<00:00, 53.82batch/s, loss=0.124]\n",
      "Epoch 37: 100%|██████████| 11718/11718 [03:39<00:00, 53.32batch/s, loss=0.124]\n",
      "Epoch 38: 100%|██████████| 11718/11718 [03:36<00:00, 54.19batch/s, loss=0.124]\n",
      "Epoch 39: 100%|██████████| 11718/11718 [03:36<00:00, 54.05batch/s, loss=0.124]\n",
      "Epoch 40: 100%|██████████| 11718/11718 [03:38<00:00, 53.53batch/s, loss=0.124]\n",
      "Epoch 41: 100%|██████████| 11718/11718 [03:36<00:00, 54.18batch/s, loss=0.124]\n",
      "Epoch 42: 100%|██████████| 11718/11718 [03:41<00:00, 52.83batch/s, loss=0.123]\n",
      "Epoch 43: 100%|██████████| 11718/11718 [03:35<00:00, 54.37batch/s, loss=0.124]\n",
      "Epoch 44: 100%|██████████| 11718/11718 [03:36<00:00, 54.03batch/s, loss=0.123]\n",
      "Epoch 45: 100%|██████████| 11718/11718 [03:38<00:00, 53.55batch/s, loss=0.123]\n",
      "Epoch 46: 100%|██████████| 11718/11718 [03:37<00:00, 53.80batch/s, loss=0.123]\n",
      "Epoch 47: 100%|██████████| 11718/11718 [03:37<00:00, 53.93batch/s, loss=0.123]\n",
      "Epoch 48: 100%|██████████| 11718/11718 [03:38<00:00, 53.63batch/s, loss=0.123]\n",
      "Epoch 49: 100%|██████████| 11718/11718 [03:36<00:00, 54.11batch/s, loss=0.123]\n",
      "Epoch 50: 100%|██████████| 11718/11718 [03:40<00:00, 53.13batch/s, loss=0.123]\n",
      "Epoch 51: 100%|██████████| 11718/11718 [03:41<00:00, 52.99batch/s, loss=0.123]\n",
      "Epoch 52: 100%|██████████| 11718/11718 [03:36<00:00, 54.18batch/s, loss=0.123]\n",
      "Epoch 53: 100%|██████████| 11718/11718 [03:39<00:00, 53.45batch/s, loss=0.123]\n",
      "Epoch 54: 100%|██████████| 11718/11718 [03:38<00:00, 53.69batch/s, loss=0.123]\n",
      "Epoch 55: 100%|██████████| 11718/11718 [03:37<00:00, 53.97batch/s, loss=0.123]\n",
      "Epoch 56: 100%|██████████| 11718/11718 [03:39<00:00, 53.33batch/s, loss=0.122]\n",
      "Epoch 57: 100%|██████████| 11718/11718 [03:37<00:00, 53.97batch/s, loss=0.122]\n",
      "Epoch 58: 100%|██████████| 11718/11718 [03:40<00:00, 53.22batch/s, loss=0.122]\n",
      "Epoch 59: 100%|██████████| 11718/11718 [03:43<00:00, 52.54batch/s, loss=0.122]\n",
      "Epoch 60: 100%|██████████| 11718/11718 [03:44<00:00, 52.20batch/s, loss=0.122]\n",
      "Epoch 61: 100%|██████████| 11718/11718 [03:48<00:00, 51.39batch/s, loss=0.122]\n",
      "Epoch 62: 100%|██████████| 11718/11718 [03:54<00:00, 50.03batch/s, loss=0.122]\n",
      "Epoch 63: 100%|██████████| 11718/11718 [03:55<00:00, 49.67batch/s, loss=0.122]\n",
      "Epoch 64: 100%|██████████| 11718/11718 [03:58<00:00, 49.13batch/s, loss=0.122]\n",
      "Epoch 65: 100%|██████████| 11718/11718 [03:58<00:00, 49.19batch/s, loss=0.122]\n",
      "Epoch 66: 100%|██████████| 11718/11718 [04:02<00:00, 48.34batch/s, loss=0.122]\n",
      "Epoch 67: 100%|██████████| 11718/11718 [04:05<00:00, 47.80batch/s, loss=0.122]\n",
      "Epoch 68: 100%|██████████| 11718/11718 [04:02<00:00, 48.29batch/s, loss=0.122]\n",
      "Epoch 69: 100%|██████████| 11718/11718 [04:04<00:00, 47.84batch/s, loss=0.122]\n",
      "Epoch 70: 100%|██████████| 11718/11718 [04:03<00:00, 48.04batch/s, loss=0.122]\n",
      "Epoch 71: 100%|██████████| 11718/11718 [04:04<00:00, 47.85batch/s, loss=0.122]\n",
      "Epoch 72: 100%|██████████| 11718/11718 [04:03<00:00, 48.04batch/s, loss=0.122]\n",
      "Epoch 73: 100%|██████████| 11718/11718 [04:03<00:00, 48.09batch/s, loss=0.122]\n",
      "Epoch 74: 100%|██████████| 11718/11718 [04:08<00:00, 47.13batch/s, loss=0.122]\n",
      "Epoch 75: 100%|██████████| 11718/11718 [04:08<00:00, 47.23batch/s, loss=0.122]\n",
      "Epoch 76: 100%|██████████| 11718/11718 [04:32<00:00, 43.03batch/s, loss=0.122]\n",
      "Epoch 77: 100%|██████████| 11718/11718 [04:07<00:00, 47.40batch/s, loss=0.122]\n",
      "Epoch 78: 100%|██████████| 11718/11718 [04:06<00:00, 47.53batch/s, loss=0.122]\n",
      "Epoch 79: 100%|██████████| 11718/11718 [04:08<00:00, 47.14batch/s, loss=0.122]\n",
      "Epoch 80: 100%|██████████| 11718/11718 [04:09<00:00, 46.96batch/s, loss=0.122]\n",
      "Epoch 81: 100%|██████████| 11718/11718 [04:12<00:00, 46.35batch/s, loss=0.121]\n",
      "Epoch 82: 100%|██████████| 11718/11718 [04:08<00:00, 47.09batch/s, loss=0.121]\n",
      "Epoch 83: 100%|██████████| 11718/11718 [04:10<00:00, 46.73batch/s, loss=0.121]\n",
      "Epoch 84: 100%|██████████| 11718/11718 [04:12<00:00, 46.36batch/s, loss=0.121]\n",
      "Epoch 85: 100%|██████████| 11718/11718 [04:11<00:00, 46.61batch/s, loss=0.121]\n",
      "Epoch 86: 100%|██████████| 11718/11718 [04:14<00:00, 45.98batch/s, loss=0.121]\n",
      "Epoch 87: 100%|██████████| 11718/11718 [04:12<00:00, 46.42batch/s, loss=0.121]\n",
      "Epoch 88: 100%|██████████| 11718/11718 [04:17<00:00, 45.48batch/s, loss=0.121]\n",
      "Epoch 89: 100%|██████████| 11718/11718 [04:12<00:00, 46.48batch/s, loss=0.121]\n",
      "Epoch 90: 100%|██████████| 11718/11718 [04:14<00:00, 46.11batch/s, loss=0.121]\n",
      "Epoch 91: 100%|██████████| 11718/11718 [04:16<00:00, 45.75batch/s, loss=0.121]\n",
      "Epoch 92: 100%|██████████| 11718/11718 [04:14<00:00, 46.01batch/s, loss=0.121]\n",
      "Epoch 93: 100%|██████████| 11718/11718 [04:16<00:00, 45.64batch/s, loss=0.121]\n",
      "Epoch 94: 100%|██████████| 11718/11718 [04:17<00:00, 45.58batch/s, loss=0.121]\n",
      "Epoch 95: 100%|██████████| 11718/11718 [04:20<00:00, 45.03batch/s, loss=0.121]\n",
      "Epoch 96: 100%|██████████| 11718/11718 [04:16<00:00, 45.70batch/s, loss=0.121]\n",
      "Epoch 97: 100%|██████████| 11718/11718 [04:15<00:00, 45.93batch/s, loss=0.121]\n",
      "Epoch 98: 100%|██████████| 11718/11718 [04:19<00:00, 45.11batch/s, loss=0.121]\n",
      "Epoch 99: 100%|██████████| 11718/11718 [04:17<00:00, 45.55batch/s, loss=0.121]\n",
      "Epoch 100: 100%|██████████| 11718/11718 [04:19<00:00, 45.23batch/s, loss=0.121]\n"
     ]
    }
   ],
   "source": [
    "total_loss = train_multi_task() # return a list of loss in different epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b868865841b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters and optimizer\n",
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "}, 'Task1_multiTask.pth.tar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reptile Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_reptile = MyModel().to(device)\n",
    "model_reptile = model_reptile.double()\n",
    "model_reptile.train()\n",
    "optimizer_reptile = torch.optim.Adam(model_reptile.parameters(), lr=hyperparams['lr'])\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 169.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Arm Link Dataset Generation Finished!\n"
     ]
    }
   ],
   "source": [
    "train_dataset_reptile =  TrainingDataset(n_tasks=hyperparams['n_tasks'], set_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22cef5144b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "rng = np.random.RandomState(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reptile(dataset=train_dataset_reptile, nb_epochs=hyperparams['n_epochs']):\n",
    "    def totorch(x):\n",
    "        return ag.Variable(torch.DoubleTensor(x))\n",
    "        \n",
    "    def train_on_batch(x, y):\n",
    "        x = totorch(x)\n",
    "        y = totorch(y)\n",
    "        optimizer_reptile.zero_grad()\n",
    "        ypred = model_reptile.forward(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer_reptile.step()\n",
    "        return loss\n",
    "\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    meta_step_size = 0.1\n",
    "    n_episode = hyperparams['n_episode']\n",
    "    n_task = hyperparams['n_tasks']\n",
    "    n_timestep = hyperparams['n_timesteps']\n",
    "    history = []\n",
    "    for epoch in range(nb_epochs):\n",
    "        losses = []\n",
    "        with tqdm(range(n_task)) as t:\n",
    "            for meta_iter in t:\n",
    "                t.set_description(f\"Epoch {epoch}\")\n",
    "                x_train = dataset.q[meta_iter*n_episode*n_timestep:(meta_iter+1)*n_episode*n_timestep]\n",
    "                y_train = dataset.x[meta_iter*n_episode*n_timestep:(meta_iter+1)*n_episode*n_timestep]\n",
    "\n",
    "                weights_before = deepcopy(model_reptile.state_dict())\n",
    "                inds = rng.permutation(len(x_train))\n",
    "                for start in range(0, len(x_train), batch_size):\n",
    "                    mbinds = inds[start:start+batch_size]\n",
    "                    losses.append(train_on_batch(x_train[mbinds], y_train[mbinds]))\n",
    "                # Interpolate between current weights and trained weights from this task\n",
    "                # I.e. (weights_before - weights_after) is the meta-gradient\n",
    "                weights_after = model_reptile.state_dict()\n",
    "                outerstepsize = meta_step_size * (1 - meta_iter / n_task) # linear schedule\n",
    "                model_reptile.load_state_dict({name: \n",
    "                    weights_before[name] + (weights_after[name] - weights_before[name]) * outerstepsize \n",
    "                    for name in weights_before})\n",
    "\n",
    "        loss = sum(losses) / len(losses)\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "        history.append(loss.item())\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [02:03<00:00,  8.10it/s]\n",
      "Epoch 1:   0%|          | 1/1000 [00:00<02:13,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.461910546956995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1000/1000 [02:11<00:00,  7.60it/s]\n",
      "Epoch 2:   0%|          | 1/1000 [00:00<02:15,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.33375790139198014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1000/1000 [02:05<00:00,  7.98it/s]\n",
      "Epoch 3:   0%|          | 1/1000 [00:00<02:29,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.2998087434558558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  23%|██▎       | 230/1000 [00:29<01:39,  7.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ecf0afd03f79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_reptile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_reptile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-6c1928e25503>\u001b[0m in \u001b[0;36mtrain_reptile\u001b[1;34m(dataset, nb_epochs)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[0mmbinds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                 \u001b[1;31m# Interpolate between current weights and trained weights from this task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[1;31m# I.e. (weights_before - weights_after) is the meta-gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6c1928e25503>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7f9864575179>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_for_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yunhao\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yunhao\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yunhao\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[1;34m\"but got {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_reptile = train_reptile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters and optimizer\n",
    "torch.save({\n",
    "    'state_dict': model_reptile.state_dict(),\n",
    "    'loss': loss_reptile,\n",
    "    'optimizer' : optimizer_reptile.state_dict(),\n",
    "}, 'Task1_reptile.pth.tar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lin1): Linear(in_features=4, out_features=512, bias=True)\n",
       "  (lin2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout_for_GD): Dropout(p=0.5, inplace=False)\n",
       "  (dropout): DropoutLayer()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel().to(device)\n",
    "checkpoint = torch.load('Task1_multiTask.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e9dd1ae481486d48c9db6dc9dc7a9a7f82b64eba274b60c6e858d1cb03bdb70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
